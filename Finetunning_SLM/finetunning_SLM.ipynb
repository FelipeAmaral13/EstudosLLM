{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFNohDvfNgf-"
      },
      "source": [
        "# Ajuste Fino de LLM Open Source Para Chatbot de Atendimento ao Cliente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPNjshYNgf_"
      },
      "source": [
        "## Instalando e Carregando Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcTaAJ4xNggA",
        "outputId": "a449c7e8-996b-47cd-eea7-e58c167a2dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m147.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instalando os pacotes\n",
        "!pip install -q watermark bitsandbytes==0.46.1 datasets==4.0.0 accelerate loralib evaluate==0.4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPDcC3EqMW8M"
      },
      "outputs": [],
      "source": [
        "# Instalando os pacotes = peft==0.17.1.dev0\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8ZnzcK1NggA"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, pipeline\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset, Features, ClassLabel, Value, Sequence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35uofVwVNggA"
      },
      "outputs": [],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Finetunning LLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7B6GciPMW8P"
      },
      "outputs": [],
      "source": [
        "# Verifica a GPU\n",
        "if torch.cuda.is_available():\n",
        "    print('Número de GPUs:', torch.cuda.device_count())\n",
        "    print('Modelo GPU:', torch.cuda.get_device_name(0))\n",
        "    print('Total Memória [GB] da GPU:',torch.cuda.get_device_properties(0).total_memory / 1e9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RN3VkmsMW8P"
      },
      "source": [
        "## Definindo os Parâmetros de Quantização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nShEvRHuMW8P"
      },
      "outputs": [],
      "source": [
        "# Define os parâmetros\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                         bnb_4bit_compute_dtype = torch.float16,\n",
        "                                         bnb_4bit_quant_type = \"nf4\",\n",
        "                                         bnb_4bit_use_double_quant = True,\n",
        "                                         llm_int8_enable_fp32_cpu_offload = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axJYIKOMW8Q"
      },
      "source": [
        "Os parâmetros acima são configurações específicas para a quantização de modelos de aprendizado de máquina com o PyTorch. Aqui está uma explicação para cada um deles:\n",
        "\n",
        "**load_in_4bit (True):** Este parâmetro indica que o modelo deve ser carregado em um formato de 4 bits. Isso é  definido para reduzir o uso de memória, permitindo que modelos grandes sejam carregados em hardware com memória limitada. Quando True, o modelo é carregado com uma precisão reduzida.\n",
        "\n",
        "**bnb_4bit_compute_dtype (torch.float16):** Define o tipo de dados a ser usado para cálculos internos quando o modelo está em formato de 4 bits. O torch.float16 indica que os cálculos serão realizados usando o tipo de dados de ponto flutuante de 16 bits, que é uma boa combinação de precisão e eficiência para muitos modelos de aprendizado de máquina.\n",
        "\n",
        "**bnb_4bit_quant_type (\"nf4\"):** Especifica o tipo de quantização a ser aplicada. O \"nf4\" se refere a um tipo específico de quantização de 4 bits. Referência aqui: https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
        "\n",
        "**bnb_4bit_use_double_quant (True):** Indica se deve ser usada uma \"quantização dupla\" durante o processo de quantização. A quantização dupla pode ajudar a manter mais precisão nos dados quantizados, mas pode ter um custo computacional mais alto.\n",
        "\n",
        "**llm_int8_enable_fp32_cpu_offload (True):** Este parâmetro sugere que para operações de baixa latência (em modelos grandes como LLMs - Large Language Models), quando os cálculos estão sendo feitos em int8 (8 bits), há uma opção para descarregar (offload) algumas operações para o CPU em formato fp32 (ponto flutuante de 32 bits). Isso pode ser útil para equilibrar carga entre CPU e GPU e para lidar com operações que exigem maior precisão. Referência aqui: https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
        "\n",
        "Esses parâmetros são avançados e específicos para otimizações de desempenho e uso de memória em modelos de aprendizado de máquina, especialmente modelos grandes que podem ter restrições de hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxfUuWRkMW8R"
      },
      "source": [
        "## Carregando Modelo e Tokenizador\n",
        "\n",
        "https://huggingface.co/tiiuae/falcon-7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSDi753EMW8R"
      },
      "outputs": [],
      "source": [
        "# Modelo\n",
        "# modelo = AutoModelForCausalLM.from_pretrained(\"tiiuae/falcon-7b\",\n",
        "#                                              quantization_config = quantization_config,\n",
        "#                                              device_map = 'auto')\n",
        "\n",
        "modelo = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\",\n",
        "                                             quantization_config = quantization_config,\n",
        "                                             device_map = 'auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc67m8M7MW8S"
      },
      "outputs": [],
      "source": [
        "# Tokenizador\n",
        "tokenizador = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQmOjPTZMW8S"
      },
      "source": [
        "## Congelando os Pesos Originais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhYe46P9MW8T"
      },
      "outputs": [],
      "source": [
        "# Loop\n",
        "for param in modelo.parameters():\n",
        "    param.requires_grad = False\n",
        "    if param.ndim == 1:\n",
        "        param.data = param.data.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8o7KoXPMW8T"
      },
      "source": [
        "**param.requires_grad = False**: Isso desativa o cálculo de gradientes para cada parâmetro, o que significa que esses parâmetros não serão atualizados durante o treinamento. Isso é comumente feito quando se quer congelar certas partes de um modelo pré-treinado para transferência de aprendizado, onde apenas as camadas superiores do modelo serão treinadas.\n",
        "\n",
        "**if param.ndim == 1: param.data = param.data.to(torch.float32)**: Esta linha converte os parâmetros unidimensionais (ou seja, vetores) para o tipo de dado float32. Isso pode ser necessário se for desejado garantir que todos os parâmetros do modelo estejam no mesmo tipo de dado, especialmente se forem realizadas operações que requerem consistência de tipos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIQRI0jV3DdY"
      },
      "source": [
        "## Ativando o Checkpoint de Gradientes do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I15uWyjIMW8U"
      },
      "outputs": [],
      "source": [
        "# Ativa o recurso de checkpoint de gradientes no modelo\n",
        "modelo.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdz7fjO8MW8U"
      },
      "source": [
        "Quando o checkpoint de gradientes está habilitado, o modelo não armazena todos os valores intermediários (atividades das camadas) durante a passagem para frente (forward pass). Em vez disso, ele armazena apenas alguns pontos de verificação. Durante a passagem para trás (backward pass), os valores intermediários que não foram armazenados são recomputados a partir dos pontos de verificação. Isso reduz a quantidade de memória necessária para armazenar os valores intermediários, mas aumenta o tempo de computação, pois alguns valores precisam ser recomputados.\n",
        "\n",
        "Essa técnica é útil quando se treina modelos muito grandes que de outra forma não caberiam na memória da GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6VnJmRdMW8V"
      },
      "outputs": [],
      "source": [
        "# Habilita a técnica chamada \"checkpointing de gradiente\"\n",
        "modelo.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdU238EQMW8V"
      },
      "source": [
        "Essa técnica é útil para reduzir o consumo de memória durante o treinamento de modelos grandes. O que ela faz é salvar certos estados intermediários (checkpoints) durante a passagem para frente (forward pass) e, em seguida, durante a passagem para trás (backward pass), esses estados são usados para recomputar os gradientes, em vez de armazenar todos os estados intermediários na memória. Isso pode diminuir a quantidade de memória necessária, mas pode aumentar o tempo de computação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmszsvbj3kc8"
      },
      "source": [
        "## Ajustando a Conversão Para Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIndxTmiMW8W"
      },
      "outputs": [],
      "source": [
        "# Conversão de tensor\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6GQjpd_MW8W"
      },
      "source": [
        "A classe CastOutputToFloat é uma subclasse da classe nn.Sequential do PyTorch, que é usada para criar uma sequência de módulos (como camadas de uma rede neural). A principal funcionalidade dessa classe é converter o tipo de dados da saída de uma sequência de módulos para torch.float32 (ou seja, um tensor de ponto flutuante de 32 bits)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUBxKE-WMW8X"
      },
      "outputs": [],
      "source": [
        "modelo.lm_head = CastOutputToFloat(modelo.lm_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URwkTqf1MW8X"
      },
      "source": [
        "A linha acima está substituindo a cabeça de modelo de linguagem (lm_head) pelo módulo CastOutputToFloat que encapsula a cabeça de modelo de linguagem original. Isso significa que toda vez que a lm_head do modelo for utilizada durante a passagem para frente, sua saída será automaticamente convertida para torch.float32. Isso pode ser útil para garantir a compatibilidade de tipo em situações onde a saída da cabeça de modelo de linguagem precisa ser do tipo float32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoj771wP3rSc"
      },
      "source": [
        "## Definindo os Parâmetros do Ajuste Fino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnPM_AWnMW8Y"
      },
      "outputs": [],
      "source": [
        "# LoRa Config\n",
        "config = LoraConfig(r = 16,\n",
        "                    lora_alpha = 32,\n",
        "                    lora_dropout = 0.05,\n",
        "                    bias = \"none\",\n",
        "                    task_type = \"CAUSAL_LM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRbWQiGQ4iDV"
      },
      "source": [
        "Os parâmetros dentro de LoraConfig ajustam diferentes aspectos da configuração de um modelo LoRA, que é uma técnica de ajuste fino para modelos de aprendizado profundo. Aqui está a explicação de cada um:\n",
        "\n",
        "**r**: Este parâmetro especifica a taxa de redução, que afeta diretamente o tamanho dos parâmetros adaptativos adicionados ao modelo. Uma taxa de redução menor significa menos parâmetros a serem aprendidos, o que pode tornar o ajuste fino mais eficiente e rápido. Por outro lado, uma taxa de redução maior permite uma adaptação mais flexível ao novo conjunto de dados, mas com o custo de maior complexidade computacional.\n",
        "\n",
        "**lora_alpha**: Define o parâmetro de escala para a adaptação LoRA. lora_alpha ajusta a magnitude da atualização aplicada aos pesos do modelo original. Um valor mais alto significa que as atualizações adaptativas terão mais impacto, permitindo ajustes mais significativos no comportamento do modelo. Essa escala pode ser crucial para garantir que as mudanças introduzidas sejam relevantes para a tarefa específica sem desviar demasiadamente do conhecimento pré-aprendido.\n",
        "\n",
        "**lora_dropout**: A taxa de dropout aplicada aos parâmetros adaptativos LoRA. Dropout é uma técnica de regularização usada para prevenir o sobreajuste durante o treinamento. Ao definir lora_dropout, você especifica a probabilidade com que as conexões entre os neurônios adaptativos serão temporariamente desativadas. Isso ajuda o modelo a generalizar melhor para dados não vistos, promovendo a robustez do ajuste fino.\n",
        "\n",
        "**bias**: Define se os termos de bias serão incluídos nos ajustes LoRA. Os termos de bias adicionam um valor constante às saídas de uma camada, o que pode ajudar na otimização e na capacidade do modelo de se ajustar aos dados. Configurar bias como \"none\" significa que esses ajustes não serão aplicados, o que pode ser preferível em cenários onde a inclusão de termos de bias não oferece benefícios claros ou se deseja manter a estrutura do modelo o mais simples possível.\n",
        "\n",
        "**task_type**: Especifica o tipo de tarefa para a qual o modelo está sendo ajustado. \"CAUSAL_LM\" refere-se a Modelagem de Linguagem Causal, um cenário onde o modelo gera texto baseado no contexto anterior de forma sequencial. Isso é essencial para garantir que as adaptações LoRA sejam direcionadas para melhorar o desempenho nesse tipo específico de tarefa, otimizando o modelo para gerar respostas coerentes e contextuais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG1hvL3YMW8Y"
      },
      "outputs": [],
      "source": [
        "# Cria o modelo considerando os parâmetros LoRa\n",
        "modelo = get_peft_model(modelo, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10qAgvEjOU0d"
      },
      "outputs": [],
      "source": [
        "# Definindo a função para imprimir os parâmetros treináveis de um modelo\n",
        "def print_trainable_parameters(model):\n",
        "\n",
        "    # Inicializa a contagem de parâmetros treináveis\n",
        "    trainable_params = 0\n",
        "\n",
        "    # Inicializa a contagem de todos os parâmetros\n",
        "    all_param = 0\n",
        "\n",
        "    # Itera sobre todos os parâmetros nomeados do modelo\n",
        "    for _, param in model.named_parameters():\n",
        "\n",
        "        # Soma o número total de elementos de todos os parâmetros\n",
        "        all_param += param.numel()\n",
        "\n",
        "        # Verifica se o parâmetro é treinável\n",
        "        if param.requires_grad:\n",
        "\n",
        "            # Soma o número de elementos aos parâmetros treináveis\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    # Imprime o número de parâmetros treináveis, o total de parâmetros e a porcentagem de parâmetros treináveis\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjSucPpGMW8Z"
      },
      "outputs": [],
      "source": [
        "# Executa a função\n",
        "print_trainable_parameters(modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9mIgXeyMW8Z"
      },
      "source": [
        "## Processamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-eCF__dMW8a"
      },
      "outputs": [],
      "source": [
        "# Define o arquivo\n",
        "arquivo = open(\"dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QilUIlzjMW8a"
      },
      "outputs": [],
      "source": [
        "# Carrega o arquivo\n",
        "dados = json.load(arquivo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocYMcme2OiOF"
      },
      "outputs": [],
      "source": [
        "# Visualiza os dados\n",
        "dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eNjXEfsMW8a"
      },
      "outputs": [],
      "source": [
        "# Listas para perguntas e respostas\n",
        "perguntas = []\n",
        "respostas = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CniJIBaMW8b"
      },
      "outputs": [],
      "source": [
        "# Loop pelos dados para extrair perguntas e respostas\n",
        "for i in dados[\"perguntas\"]:\n",
        "    perguntas += [i[\"pergunta\"]]\n",
        "    respostas += [i[\"resposta\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoDsjLDrOsNR"
      },
      "outputs": [],
      "source": [
        "# Conteúdo do dataset original\n",
        "dados[\"perguntas\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHqMWUpsMW8b"
      },
      "outputs": [],
      "source": [
        "# Uma pergunta\n",
        "perguntas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaPzRhkmOsJK"
      },
      "outputs": [],
      "source": [
        "# Resposta associada à pergunta anterior\n",
        "respostas[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQoHZ4xgMW8b"
      },
      "outputs": [],
      "source": [
        "# Agora colocamos os dados no formato adequado para treinar o modelo\n",
        "dataset = Dataset.from_dict({\n",
        "    \"id\": list(range(len(perguntas))),\n",
        "    \"perguntas\": perguntas,\n",
        "    \"respostas\": respostas\n",
        "    },\n",
        "    features = Features({\n",
        "        \"id\": Value(dtype = 'string'),\n",
        "        \"perguntas\": Value(dtype = \"string\"),\n",
        "        \"respostas\": Value(dtype = \"string\")\n",
        "    }\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVYJuT4eMW8c"
      },
      "outputs": [],
      "source": [
        "# Divide os dados em treino e teste\n",
        "dataset = dataset.train_test_split(test_size = 0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHHennw0MW8c"
      },
      "outputs": [],
      "source": [
        "# Função para o merge de colunas concatenando cada pergunta com a resposta correspondente\n",
        "def merge_columns(registro):\n",
        "    registro[\"saida\"] = registro[\"perguntas\"] + \" ->: \" + registro[\"respostas\"]\n",
        "    return registro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGaLGZP_MW8c"
      },
      "outputs": [],
      "source": [
        "# Aplica a função\n",
        "dataset = dataset.map(merge_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K11Om9m_MW8d"
      },
      "outputs": [],
      "source": [
        "# Este será o formato dos dados para treinar o modelo\n",
        "dataset[\"train\"][\"saida\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lodL2pKMW8d"
      },
      "outputs": [],
      "source": [
        "# Observe que temos um id, temos a entrada (perguntas e respostas) e temos a saída (combinação de pergunta e resposta)\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ki4drCcMW8d"
      },
      "outputs": [],
      "source": [
        "# Tokenizamos os dados\n",
        "dataset = dataset.map(lambda samples: tokenizador(samples['saida']), batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfZkY-lVMW8e"
      },
      "outputs": [],
      "source": [
        "# Dados tokenizados\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4d_vj8fFLRa"
      },
      "source": [
        "## Definindo os Argumentos de Treino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8b6xwDJMW8e"
      },
      "outputs": [],
      "source": [
        "# Se não tiver pad, ajustamos com o pad do tokenizador\n",
        "if tokenizador.pad_token == None:\n",
        "    tokenizador.pad_token = tokenizador.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVLzAD21MW8e"
      },
      "outputs": [],
      "source": [
        "# Argumentos de treino\n",
        "trainer = transformers.Trainer(model = modelo,\n",
        "                                   train_dataset = dataset[\"train\"],\n",
        "                                   eval_dataset = dataset[\"test\"],\n",
        "                                   args = transformers.TrainingArguments(eval_strategy = \"epoch\",\n",
        "                                                                         per_device_train_batch_size = 2,\n",
        "                                                                         gradient_accumulation_steps = 2,\n",
        "                                                                         num_train_epochs = 10,\n",
        "                                                                         learning_rate = 2e-4,\n",
        "                                                                         fp16 = True,\n",
        "                                                                         logging_steps = 1,\n",
        "                                                                         output_dir = 'outputs',\n",
        "                                                                         report_to = \"none\"),\n",
        "                                   data_collator = transformers.DataCollatorForLanguageModeling(tokenizador, mlm = False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_fqAn6JBKtU"
      },
      "source": [
        "Aqui está a explicação dos componentes principais na configuração de treinamento usando a classe Trainer do transformers:\n",
        "\n",
        "**model**: Este argumento especifica o modelo que será treinado. Aqui, modelo é uma instância de um modelo pré-carregado ou customizado que você deseja treinar ou ajustar fino para uma tarefa específica.\n",
        "\n",
        "**train_dataset**: Define o conjunto de dados de treinamento. dataset[\"train\"] indica que estamos utilizando a parte de treinamento de um conjunto de dados chamado dataset.\n",
        "\n",
        "**eval_dataset**: Define o conjunto de dados de avaliação (teste). dataset[\"test\"] refere-se à parte de teste do mesmo conjunto de dados, utilizado para avaliar o desempenho do modelo após cada época de treinamento, dependendo da estratégia de avaliação definida.\n",
        "\n",
        "**args**: Uma instância de transformers.TrainingArguments que contém vários argumentos de configuração para o processo de treinamento:\n",
        "\n",
        "**evaluation_strategy = \"epoch\"**: A avaliação do modelo acontece ao final de cada época.\n",
        "\n",
        "**per_device_train_batch_size = 2**: Define o tamanho do lote (batch size) para o treinamento em cada dispositivo (por exemplo, GPU).\n",
        "\n",
        "**gradient_accumulation_steps = 2**: O número de passos de acumulação de gradiente antes de realizar uma etapa de otimização. Isso efetivamente aumenta o tamanho do lote ao custo de memória reduzido, já que os gradientes são acumulados em várias etapas antes da atualização dos pesos.\n",
        "\n",
        "**num_train_epochs = 5**: O número de épocas de treinamento.\n",
        "\n",
        "**learning_rate = 2e-4**: A taxa de aprendizagem inicial.\n",
        "\n",
        "**fp16 = True**: Habilita o treinamento usando precisão mista (floating point 16), o que pode acelerar o treinamento e reduzir o uso de memória em GPUs compatíveis.\n",
        "\n",
        "**logging_steps = 1**: Frequência (em número de passos de treinamento) para logar as métricas de treinamento.\n",
        "\n",
        "**output_dir = 'outputs'**: O diretório onde os artefatos de treinamento (como modelos salvos) serão armazenados.\n",
        "\n",
        "**data_collator**: Especifica como os lotes de dados são formados ou colados juntos. transformers.DataCollatorForLanguageModeling é usado para a modelagem de linguagem, onde tokenizador é o tokenizador a ser usado para preparar os lotes de treinamento, e mlm = False indica que a modelagem de linguagem não é mascarada (ou seja, é uma modelagem de linguagem causal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOUj3hQyFPuE"
      },
      "source": [
        "## Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLc2qhgvMW8f"
      },
      "outputs": [],
      "source": [
        "# Não usa o cache\n",
        "modelo.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-CL2s031rKw"
      },
      "outputs": [],
      "source": [
        "# Treinamento e ajuste fino do modelo\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4xe7tkiGJEL"
      },
      "source": [
        "## Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkkmBN3jMW8f"
      },
      "outputs": [],
      "source": [
        "# Função para extrair as previsões\n",
        "def predict(question):\n",
        "\n",
        "    # Garante que não estamos em modo treino\n",
        "    modelo.eval()\n",
        "\n",
        "    # Identifica cpu ou cuda (GPU)\n",
        "    device = next(modelo.parameters()).device\n",
        "\n",
        "    # tokeniza e já envia para o mesmo dispositivo do modelo\n",
        "    batch = tokenizador(f\"{question} ->: \",\n",
        "                        return_tensors='pt',\n",
        "                        padding=True,\n",
        "                        truncation=True)\n",
        "\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Sem gradiente e com autocast em GPU para fp16\n",
        "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "        output_tokens = modelo.generate(\n",
        "            **batch,\n",
        "            max_new_tokens=50,\n",
        "            pad_token_id=tokenizador.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decodifica de volta para string\n",
        "    return tokenizador.decode(output_tokens[0], skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQii7jQiMW8f"
      },
      "outputs": [],
      "source": [
        "# Lista para as previsões\n",
        "previsoes = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1aYvWxiMW8g"
      },
      "outputs": [],
      "source": [
        "# Loop para extrair as previsões\n",
        "for i in dataset[\"test\"][\"perguntas\"]:\n",
        "    previsoes.append(predict(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGrSrb_GPy1"
      },
      "source": [
        "## Interpretando a Métrica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "931ReWZdMW8g"
      },
      "outputs": [],
      "source": [
        "# Carrega o módulo da métrica\n",
        "bleu = evaluate.load('bleu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIkvEEkQC7Jd"
      },
      "source": [
        "A métrica BLEU (Bilingual Evaluation Understudy) é uma das métricas mais conhecidas e utilizadas em Processamento de Linguagem Natural (PLN) para avaliar a qualidade de textos gerados por máquinas. Desenvolvida por Papineni et al. em 2002, a métrica BLEU compara um texto gerado automaticamente com um ou mais textos de referência (geralmente humanos) para medir a qualidade da produção da máquina.\n",
        "\n",
        "A essência da métrica BLEU é medir quão similares são os n-gramas (sequências contínuas de n itens de um dado texto) do texto gerado automaticamente em comparação aos n-gramas dos textos de referência. O cálculo da pontuação BLEU envolve os seguintes passos principais:\n",
        "\n",
        "**Cálculo de Precisão de N-gramas**: Para cada n-grama no texto gerado, verifica-se quantos desses n-gramas também aparecem nos textos de referência. A precisão é então calculada como o número de n-gramas coincidentes dividido pelo número total de n-gramas no texto gerado.\n",
        "\n",
        "**Punição por Sentenças Curtas (Brevity Penalty, BP)**: Para evitar que a métrica favoreça respostas indevidamente curtas (que podem ter alta precisão de n-gramas mas são inúteis), o BLEU inclui uma punição para textos gerados que são mais curtos que os textos de referência. Essa punição diminui a pontuação BLEU de textos muito curtos, equilibrando a precisão e a completude da resposta.\n",
        "\n",
        "**Combinação de Precisões com Pesos**: A pontuação BLEU final geralmente combina as precisões de n-gramas de diferentes tamanhos (por exemplo, de 1 a 4), aplicando pesos a cada uma dessas precisões. Isso é feito para considerar a fluidez e a estrutura gramatical do texto, além da presença de palavras-chave.\n",
        "\n",
        "A pontuação final do BLEU é então calculada usando uma média geométrica dessas precisões ponderadas, ajustada pela punição por sentenças curtas. A pontuação varia de 0 a 1, onde 1 indica uma correspondência perfeita com o texto de referência, embora frequentemente seja expressa em porcentagem (0 a 100).\n",
        "\n",
        "Embora o BLEU seja amplamente utilizado devido à sua simplicidade e capacidade de avaliação rápida, ele tem limitações, como a incapacidade de avaliar a adequação semântica ou a fluidez gramatical de maneira mais profunda. Por isso, outras métricas complementares, como ROUGE, METEOR e outras, também são usadas para avaliar a qualidade da geração de texto em PLN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6XBpwBRMW8g"
      },
      "outputs": [],
      "source": [
        "# Extrai os dados reais\n",
        "dados_reais = dataset[\"test\"][\"saida\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8VQ_hYGMW8g"
      },
      "outputs": [],
      "source": [
        "# Calcula a métrica comparando valores reais e previsões\n",
        "resultado = bleu.compute(predictions = previsoes, references = dados_reais)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b_aAkQSMW8h"
      },
      "outputs": [],
      "source": [
        "resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4mSpcgaFFlK"
      },
      "source": [
        "Esses resultados representam a avaliação de um texto gerado usando a métrica BLEU. Cada componente do resultado oferece uma visão diferente da qualidade da tradução:\n",
        "\n",
        "**bleu**: Esta é a pontuação BLEU geral do texto traduzido em relação ao(s) texto(s) de referência. A pontuação varia de 0 a 1, onde valores mais altos indicam melhor correspondência com o texto de referência. Uma pontuação de aproximadamente 0.19 sugere que, embora haja algum grau de correspondência, a qualidade do texto gerado tem bastante espaço para melhorias.\n",
        "\n",
        "**precisions**: Estas são as precisões dos n-gramas de 1 a 4-gramas, respectivamente. A precisão para unigramas é 0.375, indicando que 37.5% dos unigramas na tradução coincidem com aqueles nos textos de referência. As precisões diminuem para n-gramas maiores, o que é esperado, pois é mais difícil conseguir correspondências exatas para sequências mais longas de palavras. A precisão mais baixa para 4-gramas (aproximadamente 0.133) sugere dificuldades em capturar estruturas de frases mais longas ou expressões idiomáticas corretamente.\n",
        "\n",
        "**brevity_penalty**: A penalidade por brevidade (BP) ajusta a pontuação BLEU para traduções que são significativamente mais curtas que seus textos de referência. Uma BP próxima de 1, como neste caso, indica que a diferença de comprimento entre a tradução e o texto de referência é mínima, portanto, a penalidade aplicada é baixa.\n",
        "\n",
        "**length_ratio**: Este é o ratio entre o comprimento da tradução e o comprimento do texto de referência. Um valor de aproximadamente 0.98 sugere que o comprimento da tradução é quase igual ao do texto de referência, corroborando a baixa brevidade da penalidade.\n",
        "\n",
        "**translation_length**: O comprimento total da tradução gerada, medido em número de palavras.\n",
        "\n",
        "**reference_length**: O comprimento total do texto de referência, também em número de palavras. A proximidade entre os comprimentos da tradução e da referência confirma que a penalidade por brevidade foi mínima.\n",
        "\n",
        "Em resumo, os resultados sugerem que, embora o texto gerado tenha um comprimento adequado em comparação ao texto de referência e uma razoável correspondência de unigramas, há uma queda significativa na precisão à medida que se consideram sequências mais longas de palavras. Isso indica que, enquanto o texto gerado pode conter muitas palavras corretas individualmente, ela pode estar enfrentando dificuldades em capturar a estrutura correta das frases e a fluidez do texto de referência. Isso se deve ao baixo volume de dados usados no ajuste fino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP9sC8YEK2P6"
      },
      "source": [
        "## Deploy e Uso do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtuFigq711l7"
      },
      "outputs": [],
      "source": [
        "# Define o device\n",
        "device = next(modelo.parameters()).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqvAMUQyMW8h"
      },
      "outputs": [],
      "source": [
        "nova_pergunda_usuario = 'Como posso criar uma conta?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLbboOAZ2PtE"
      },
      "outputs": [],
      "source": [
        "pergunta_tokenizada = tokenizador(nova_pergunda_usuario, return_tensors = \"pt\", padding = True, truncation = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CFJJfkd2Xmw"
      },
      "outputs": [],
      "source": [
        "# Move o batch para o mesmo dispositivo do modelo\n",
        "pergunta_tokenizada = {nome: tensor.to(device) for nome, tensor in pergunta_tokenizada.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrJhtK7-MW8i"
      },
      "outputs": [],
      "source": [
        "pergunta_tokenizada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3MUZ-fJ2foc"
      },
      "outputs": [],
      "source": [
        "# Gera a previsão sem cálculo de gradiente e com autocast\n",
        "with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "    previsao_tokens = modelo.generate(**pergunta_tokenizada, max_new_tokens = 50, pad_token_id = tokenizador.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kU5eaNd3OTv"
      },
      "source": [
        "O autocast é um mecanismo do PyTorch para Mixed Precision Training que, ao ser ativado com o contexto faz com que o framework escolha automaticamente, para cada operador, se ele deve rodar em meia‑precisão (FP16) ou em precisão completa (FP32). Na prática, isso significa que as partes do cálculo que podem tolerar a menor precisão são executadas em FP16, o que reduz o uso de memória da GPU e acelera os kernels, enquanto operações mais sensíveis à perda de precisão continuam em FP32.\n",
        "\n",
        "Para manter a estabilidade numérica durante o backward, o autocast costuma ser combinado com um GradScaler, que escala dinamicamente os gradientes para evitar underflow em FP16. Tudo isso é feito quase sem alterações no seu código de treinamento: basta envolver a passagem de forward no bloco autocast e, se desejar, emparelhar com o escalador de gradientes para garantir que a redução de precisão não afete a convergência do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrKrlXcQ0is8"
      },
      "outputs": [],
      "source": [
        "# Decode da saída\n",
        "tokenizador.decode(previsao_tokens[0], skip_special_tokens = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTJ031h8gzvW"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a \"Finetunning LLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaE9Tu_tgz2f"
      },
      "outputs": [],
      "source": [
        "%watermark -v -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPiG_NTZgz6z"
      },
      "outputs": [],
      "source": [
        "%watermark --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaVpA983NggC"
      },
      "source": [
        "# Fim"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
